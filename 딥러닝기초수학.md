## 딥러닝 기초 수학
#### 손실 함수 (Cost Function, Loss Function)
- 예측한 값을 나타낸 실선과 실제 값을 나타낸 점 사이의 오차가 좁을수록 정확도가 높다.
- 격차가 음인지 양인지는 중요하지 않고 절대값이 중요함
- 예측한 값 H(x) = Wx + b (W는 기울기, b는 y절편)
- 즉 예측한 값과 실제값 사이의 거리를 측정하는 것이 손실 함수이다.
- (예측한 값-실제값)을 제곱해서 다 더하고 나눠서 평균을 구함 -> m은 데이터의 개수
- (예측한 값-실제값)을 제곱하여 음수를 없애고 격차를 커지게 만들어서 극대화

#### 미분
= 도함수
- 도함수는 미분계수를 쉽게 찾을 수 있도록 매핑해준 것
- 미분 또는 도함수가 의미하는 것은, 어떤 한 점에서 그릴 수 있는 접선의 기울기로 순간적인 변화율을 뜻한다.
- 미분 결과는 미분 계수 = 도함수의 값
- 접선의 기울기를 구하는 이유는 해당 점에서 함수의 진행 방향, 변화의 정도를 알기 위해서.
  - 기울기는 얼마나 어느 방향으로 움직이냐에 대한 정보를 담고 있음
  - 기울기가 양수면 = 경사가 가파르면 x에 따른 y의 값이 빠르게 상승
  - 기울기가 음수면 = 경사가 가파르면 x에 따른 y의 값이 빠르게 하강
  - 경사가 평평하면 = 기울기가 0이면 이 지점이 최소점 = 극점을 의미함. 이 함수의 최솟값을 구하고 싶다면 미분을 해서 기울기가 0인 지점을 구하면 됨
  - 평균적인 변화율을 계속해서 줄여나가면 그것이 바로 미분값이다.
  - 상수는 변하지 않으므로 미분해도 0인 것

#### 연쇄법칙
- 체인을 안 쓰고 거의 미분 못함
- 접하는 함수의 대부분을 합성 함수라고 할 수 있음

### 편미분(Partial Differentiation)
- 학습 알고리즘, 경사하강법, 역전파
- f(x1, x2) 는 x1, x2 둘다 영향을 받는데, x1에서만 변화를 보고 싶다면 x2를 상수화 시켜준다. 이에 따라 미분은 자동적으로 x1에만 들어감
- 이때, x1, x2 모두에 대한 변화를 보고 싶다면 정미분이라는 개념을 추가
- 편미분한 것을 다 모으면 기울기 벡터
- 즉, x가 하나일 땐 미분이지만 x가 여러개면 편미분이다.

1. 선형회귀 (Linear Regression)
   - 보통 회귀는 선형회귀를 의미
   - x라는 변수에 대해 y가 움직이고 있는데 둘 간의 관계를 보고자 하는 것 -> y= ax+b 라는 선형 함수로 정의
   - a, b에 따라 선형 함수가 무수히 많아지는데 그 중 관계를 제일 잘 설명한 하나의 선형함수를 보고자 H(x) 라는 가설회귀선을 만듦
   - 실제값과 오차가 가장 작은 회귀선을 골라야 하며, 이 오차가 회귀선을 고르는 기준이 됨
   - 오차는 결국 예측한 y 데이터에서 실제 y 값을 빼서 제곱한 것을 평균 낸 것
   - 트레이닝 데이터에서 보여지듯 공부시간에 대한 값 입력에 대해서 결과값인 시험성적이 연속적
   - 선형 회귀의 목적은 가장 작은 값을 가지는 손실함수의 W와 b를 가지는 것, Loss 를 0에 가깝게 낮춰야 한다.
2. 로지스틱회귀 (Rogistic Regression)
   - 회귀로 풀 수 없는 문제, 분류로 풀어야 함
   - 0 아니면 1로 분류, Signoid 함수가 나옴
   - 입력값은 선형회귀와 동일하더라도 결과를 이산적인 분류값으로 나타냄
3. 경사하강법
   - 기울기가 크면 가파르므로 빨리 내려오듯 기울기 값에는 크기가 있고 음의 방향인지 양의 방향인지 알 수 있음
   - 기울기 값에는 크기와 방향과 속도가 모두 들어 있음
   - 현재 Loss가 현재 고도
   - 가장 예측을 잘하는 건 고도가 0에 가까운 것으로 Loss 가 크면 하강하면서 내려와야함
   - 어떤 방향으로 내려갈 것인가 -> cost 가 점점 줄어드는 방향으로 경사도를 따지는게 편미분에서 따짐
   - 현재 고도에서 Back Propagation 을 진행 -> Weight와 Bias 가 나오고 반복하면서 정확한 지점을 찾아냄
     - Back Propagation 은 가장 Loss가 낮아지는 최적의 Weight과 Bias 를 찾아내는 방법론
   - 선형을 여러번 긋는다 = 학습해서 (Weight와 Bias 업데이트) Loss 를 보고, 학습과 Loss 를 반복
   - Loss Function이 0이 되거나 0 근처에 갔을 때 가장 정확한 W와 b를 찾아낼 수 있음
   1. 배치 경사하강법 : 모든 m개의 샘플에 대한 경사의 합만큼 변수를 업데이트
   2. 확률적 경사하강법 : 임의의 1개의 샘플에 대한 경사만큼 변수를 업데이트
   3. 미니배치 경사하강법 : 임의의 n개의 샘플에 대한 경사의 합만큼 변수를 업데이트

### 선형대수 : 벡터/다차원 행렬 연산
1. 텐서 : 딥러닝의 데이터 타입
   - 스칼라 0차원 텐서 / 벡터 1차원 텐서 / 행렬 2차원 텐서
   - 첫번째 차원은 샘플, 예를 들어 (10, 128, 128, 3) 은 샘플 개수가 10개인 128x128 3개의 채널인 RGB 이미지
2. 텐서 연산
   - 원소별 max, 브로드캐스팅 합, 행렬곱 이 있음
   ```
   keras.layers.Dense(32, activation='relu'): output = relu(dot(X, W) + b)
   텐서 X와 W의 행렬곱 -> 결과 행렬과 벡터 b간 브로드캐스팅 합 -> 결과 행렬 원소별 max(Z, 0)
   ```

   ```
   import numpy as np
   X = np.random((10, 128) // X.shape: (10, 128)
   W = np.random((128, 32) // W.shape: (128, 32)
   b = np.ones((10, 1)) // b.shape: (10, 1)

   // 행렬곱
   Z0 = np.dot(X, W) // Z0.shape: (10, 32)

   // 브로드캐스팅 합
   Z1 = Z0 + b // Z1.shape: (10, 32)

   // 원소별 max
   Z = np.maximum(Z1, 0) // Z.shape :(10, 32)
   ```
3. 넘파이
   - 파이썬 대표 오픈소스 선형대수 라이브러리
   - 고성능 벡터/메트릭스 연산 및 바이너리 데이터 타입 지원
   ```
   pip install -U numpy
   ```
   - 객체 : ndarray : N차원 행렬
