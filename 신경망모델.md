# 신경망 모델

## 신경망
- 입력 변수와 출력 변수의 관계를 여러 층으로 이루어진 뉴런들의 네트워크로 표현
- 이미지/음성/언어/센서 데이터 모델링에서 최고의 성능

#### 선형/로지스틱 회귀 알고리즘과 비교
- 입력층과 출력층 사이에 은닉층이 추가
- 은닉층은 여러 뉴런 포함
- 각 뉴런은 이전 층의 출력 벡터에 계수 행렬을 곱한 후 상수 벡터를 더하여 활성 함수 적용한 값을 출력
- 역전파 알고리즘으로 계수 값 학습

1. 1-뉴런 신경망 (선형/로지스틱 회귀)
2. 2층 신경망
3. 다층 신경망

### 케라스 예제
```
// InputLayer -> Dense
  model = Sequential([
    InputLayer(input_shape = (2, )),
    Dense(1, activation = 'relu')
  ])
  model.compile(loss='mean_squared_error', optimizer='adam')
  model.summary()
```


## 심층 신경망 모델
심층 신경망의 기본 개념은 2000년 이전에 정립하여 2012년부터 활성화 됨
- LeNet 1998 : 7층 6만개의 변수, ATM 손글씨 인식에 사용
- AlexNet 2012 : 8층 6천만개 변수, 이미지넷 대회 1위, GPU를 사용한 첫 사례
- VGGNet 2014 : 16층 1억개 변수, 이미지넷 대회 2위
- ResNet 2015 : 152층 2천만개 변수, 이미지넷 1위

### 자연어 처리
- Seq2Seq 2014 : 구글 브레인의 RNN 기반 기계번역 모델 구조
- 트랜스포머 2017 : 구글 브레인의 어텐션 기반 모델 구조
- BERT 2019 : 구글 AI 트랜스포머 기반 모델, 3억 4500만개 변수
- GPT-3 2021 : 오픈 AI 트랜스포머 기반 모델, 1750억개 변수

### 주요 은닉층
- Dense : 기본 층, 입력 값에 계수 행렬을 곱한 후 상수 벡터를 더한 값을 출력
- Dropout : 학습 시 일정 비율로 랜덤하게 뉴론의 출력의 억제
- BatchNormalization / LayerNormalization : 층의 입력 값을 정규화
- MaxPooling1D / AveragePooling1D : 입력 값 중 최대/평균 값을 선택
- Embedding : 정수를 실수 벡터로 변환
- Conv1D / Conv2D : 1차원/2차원 Convolution 연산
- LSTM/GRU : 순환 신경망 층
- MultiHeadAttention : 어텐션 층

## 역전파 학습
신경망에서 경사하강법을 위해 각 층의 계수에 대한 손실 함수의 경사값을 구하는 법
<br/> 연쇄법칙을 사용하여 각 계수에 대한 손실 함수의 편미분 값을 계산
